# ğŸ” Guia de ValidaÃ§Ã£o - Sentry AI Monitoring v2

## âœ… Status da InstrumentaÃ§Ã£o

**Fase 1**: Harvey Specter + Backend âœ… COMPLETO
- Frontend: `createChatSpan()` em `useAIStreaming`
- Backend: `createBackendChatSpan()` em `api/llm-stream.ts`
- Commits: `313af69`, `d8ce630`, `d7df610`, `7270736`

---

## ğŸ“Š ValidaÃ§Ã£o Manual no Dashboard Sentry.io

### Passo 1: Acessar Dashboard

1. **Login no Sentry**: https://sentry.io
2. **Selecionar projeto**: `assistente-juridico-p`
3. **Navegar**: `Insights` â†’ `AI` â†’ `AI Agents`

### Passo 2: Verificar Harvey Specter (Frontend)

**Filtros recomendados**:
```
gen_ai.agent.name = "harvey-specter"
gen_ai.system = "gcp.gemini"
conversation.session_id IS NOT NULL
```

**Spans esperados** (por conversaÃ§Ã£o):
- **Operation**: `gen_ai.chat`
- **Name**: `Harvey Specter Chat`
- **Duration**: 2-10 segundos (dependendo da resposta)

**Atributos que devem aparecer**:
```json
{
  "gen_ai.system": "gcp.gemini",
  "gen_ai.request.model": "gemini-2.5-pro",
  "gen_ai.operation.name": "chat",
  "gen_ai.agent.name": "harvey-specter",
  "gen_ai.request.temperature": 0.8,
  "gen_ai.request.max_tokens": 2000,
  "conversation.session_id": "uuid-v4",
  "conversation.turn": 1, // incrementa a cada mensagem
  "gen_ai.request.messages": "[{\"role\":\"user\",\"content\":\"...\"}]"
}
```

**VerificaÃ§Ãµes**:
- âœ… Cada mensagem do usuÃ¡rio gera 1 span
- âœ… `conversation.turn` incrementa sequencialmente (1, 2, 3...)
- âœ… `conversation.session_id` Ã© o mesmo para toda a conversa
- âœ… Mensagens do usuÃ¡rio aparecem em `gen_ai.request.messages`

### Passo 3: Verificar Backend (Server-Side)

**Filtros recomendados**:
```
server.side = true
vercel.function = "llm-stream"
gen_ai.system IN ["openai", "gcp.gemini"]
```

**Spans esperados**:
- **Operation**: `gen_ai.chat`
- **Name**: `Backend LLM Streaming - OpenAI` ou `Backend LLM Streaming - Gemini`
- **Duration**: 2-10 segundos

**Atributos que devem aparecer**:
```json
{
  "gen_ai.system": "gcp.gemini", // ou "openai"
  "gen_ai.request.model": "gemini-2.5-pro", // ou "gpt-4o-mini"
  "server.side": true,
  "vercel.function": "llm-stream",
  "stream.completed": true,
  "gen_ai.request.messages": "[{\"role\":\"user\",\"content\":\"...\"}]"
}
```

**VerificaÃ§Ãµes**:
- âœ… Spans server-side aparecem separados dos frontend
- âœ… `server.side: true` confirma origem backend
- âœ… `stream.completed: true` indica conclusÃ£o bem-sucedida
- âœ… Erros capturam `span.setStatus(2)` com stacktrace

### Passo 4: CorrelaÃ§Ã£o Frontend â†” Backend

**Como verificar**:
1. Enviar mensagem no Harvey Specter Chat
2. Buscar span frontend por `conversation.session_id`
3. Buscar span backend no mesmo intervalo de tempo
4. Verificar se timestamps coincidem (Â±1-2 segundos)

**Esperado**:
```
Timeline:
[Frontend] gen_ai.chat (Harvey Specter) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â””â”€> [Backend] gen_ai.chat (llm-stream) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Atributos para correlaÃ§Ã£o**:
- `conversation.session_id` (frontend)
- `gen_ai.request.messages` (deve ser idÃªntico)
- Timestamp de inÃ­cio (frontend) â‰ˆ Timestamp de inÃ­cio (backend)

---

## ğŸ§ª Teste Passo a Passo

### PreparaÃ§Ã£o

```bash
# 1. Dev server deve estar rodando
npm run dev
# Abre em: http://localhost:5175

# 2. Verificar se Sentry estÃ¡ configurado
# Verificar se SENTRY_DSN estÃ¡ em .env
```

### Teste 1: ConversaÃ§Ã£o BÃ¡sica (Frontend)

**AÃ§Ã£o**:
1. Abrir Harvey Specter Chat
2. Enviar mensagem: "OlÃ¡ Harvey, como vai?"
3. Aguardar resposta
4. Enviar segunda mensagem: "Me explique sobre prazos processuais"
5. Aguardar resposta

**ValidaÃ§Ã£o no Sentry**:
- âœ… 2 spans criados
- âœ… `conversation.turn = 1` no primeiro
- âœ… `conversation.turn = 2` no segundo
- âœ… Mesmo `conversation.session_id` em ambos
- âœ… DuraÃ§Ã£o razoÃ¡vel (2-10s)
- âœ… Sem erros

### Teste 2: Streaming Backend

**AÃ§Ã£o**:
1. Enviar mensagem longa: "Harvey, redija uma petiÃ§Ã£o inicial sobre..."
2. Observar resposta sendo streamada em tempo real
3. Aguardar conclusÃ£o completa

**ValidaÃ§Ã£o no Sentry**:
- âœ… Span backend com `server.side: true`
- âœ… `stream.completed: true`
- âœ… Modelo correto: `gemini-2.5-pro`
- âœ… `gen_ai.request.messages` contÃ©m prompt completo

### Teste 3: Erro de Backend

**AÃ§Ã£o**:
1. Simular erro: Desabilitar `GEMINI_API_KEY` temporariamente
2. Enviar mensagem no Harvey
3. Aguardar erro

**ValidaÃ§Ã£o no Sentry**:
- âœ… Span marcado como erro (vermelho)
- âœ… `span.status = 2` (error)
- âœ… Stacktrace disponÃ­vel
- âœ… Mensagem de erro: "Gemini error: 401..."

### Teste 4: MÃºltiplas Conversas

**AÃ§Ã£o**:
1. Abrir 3 abas diferentes
2. Iniciar conversaÃ§Ã£o em cada uma
3. Enviar mensagens diferentes

**ValidaÃ§Ã£o no Sentry**:
- âœ… 3 `conversation.session_id` diferentes
- âœ… Spans nÃ£o se misturam entre conversas
- âœ… Cada conversa tem seu prÃ³prio `turn` counter

---

## ğŸ“ˆ MÃ©tricas Esperadas

### Performance

| MÃ©trica | Valor Esperado |
|---------|----------------|
| **LatÃªncia Frontend** | 50-200ms (apenas wrapping) |
| **LatÃªncia Backend** | 2-10s (LLM completo) |
| **Overhead Sentry** | <5% (negligÃ­vel) |
| **Taxa de Erro** | <1% |

### Cobertura

| Componente | Status |
|------------|--------|
| Harvey Specter Chat | âœ… 100% |
| Backend OpenAI | âœ… 100% |
| Backend Gemini | âœ… 100% |
| Mrs. Justin-e | â³ Fase 2 |
| RedaÃ§Ã£o de PetiÃ§Ãµes | â³ Fase 2 |
| Monitor DJEN | â³ Fase 2 |

---

## ğŸš€ Deploy em ProduÃ§Ã£o

### VerificaÃ§Ãµes PrÃ©-Deploy

```bash
# 1. TypeScript compila
npm run type-check
# âœ… Sem erros

# 2. ESLint OK
npm run lint
# âœ… Max 150 warnings

# 3. Build OK
npm run build
# âœ… dist/ gerado

# 4. VariÃ¡veis de ambiente
cat .env.production
# âœ… SENTRY_DSN configurado
```

### Deploy Vercel

```bash
# Deploy automÃ¡tico via GitHub
git push origin main
# âœ… Vercel detecta push
# âœ… Build em SÃ£o Paulo (gru1)
# âœ… Deploy em https://assistente-juridico-github.vercel.app
```

### Monitoramento PÃ³s-Deploy

**URLs para monitorar**:
- **App**: https://assistente-juridico-github.vercel.app
- **Health**: https://assistente-juridico-github.vercel.app/api/health
- **Sentry Dashboard**: https://sentry.io/insights/ai/

**VerificaÃ§Ãµes**:
1. Acessar app em produÃ§Ã£o
2. Testar Harvey Specter Chat
3. Enviar 3 mensagens
4. Aguardar 5 minutos
5. Verificar spans no Sentry
6. Confirmar `server.side: true` em spans backend

**Alertas esperados**:
- âœ… Nenhum erro crÃ­tico
- âœ… LatÃªncia <10s
- âœ… Taxa de sucesso >99%

---

## ğŸ› Troubleshooting

### Problema: Spans nÃ£o aparecem no Sentry

**Causas possÃ­veis**:
1. `SENTRY_DSN` nÃ£o configurado
2. Sentry nÃ£o inicializado
3. Firewall bloqueando sentry.io

**SoluÃ§Ã£o**:
```typescript
// Verificar em src/main.tsx
import * as Sentry from '@sentry/react';

Sentry.init({
  dsn: config.sentry.dsn,
  integrations: [/* ... */],
});

// Testar manualmente
Sentry.captureMessage('Test from console');
```

### Problema: Backend spans nÃ£o correlacionam com frontend

**Causas possÃ­veis**:
1. `conversation.session_id` nÃ£o sendo passado
2. Timestamps dessincronizados
3. Spans em projetos Sentry diferentes

**SoluÃ§Ã£o**:
- Verificar se frontend passa `sessionId` no request
- Sincronizar relÃ³gios (NTP)
- Confirmar mesmo projeto Sentry

### Problema: Atributos faltando

**Causas possÃ­veis**:
1. Atributos nÃ£o setados no cÃ³digo
2. Valores undefined/null
3. StringificaÃ§Ã£o JSON falhou

**SoluÃ§Ã£o**:
```typescript
// Sempre validar antes de setar
if (messages && messages.length > 0) {
  span?.setAttribute('gen_ai.request.messages', JSON.stringify(messages));
}
```

---

## âœ… Checklist Final

### Frontend (Harvey Specter)
- [ ] Spans aparecem com `gen_ai.agent.name = "harvey-specter"`
- [ ] `conversation.session_id` Ãºnico por conversa
- [ ] `conversation.turn` incrementa corretamente
- [ ] Mensagens do usuÃ¡rio em `gen_ai.request.messages`
- [ ] Sem erros no console browser

### Backend (api/llm-stream.ts)
- [ ] Spans com `server.side: true`
- [ ] `vercel.function = "llm-stream"`
- [ ] `stream.completed: true` ao finalizar
- [ ] Erros capturam stacktrace completo
- [ ] Modelo correto (`gemini-2.5-pro` ou `gpt-4o-mini`)

### ProduÃ§Ã£o (Vercel)
- [ ] Deploy bem-sucedido
- [ ] Health check OK: `/api/health`
- [ ] Spans aparecem no Sentry em produÃ§Ã£o
- [ ] CorrelaÃ§Ã£o frontendâ†”backend funcionando
- [ ] LatÃªncia aceitÃ¡vel (<10s)

---

## ğŸ“š Recursos

- **DocumentaÃ§Ã£o oficial**: https://docs.sentry.io/platforms/javascript/guides/react/ai-agents/
- **Dashboard**: https://sentry.io/insights/ai/
- **CÃ³digo**: `src/lib/sentry-gemini-integration-v2.ts`
- **Guia completo**: `docs/SENTRY_AI_MONITORING.md`
- **Testes**: `scripts/test-sentry-instrumentation.ts`

---

**Ãšltima atualizaÃ§Ã£o**: 5 de dezembro de 2025
**Autor**: GitHub Copilot
**Status**: âœ… Fase 1 Completa - Pronto para validaÃ§Ã£o
