# Sentry Google Gen AI Integration

IntegraÃ§Ã£o automÃ¡tica do **Google Gemini 2.5 Pro** com **Sentry AI Monitoring**, adaptada da [integraÃ§Ã£o Python oficial](https://docs.sentry.io/platforms/python/integrations/google-genai/).

## ðŸ“Š VisÃ£o Geral

Esta integraÃ§Ã£o conecta automaticamente todas as chamadas ao Gemini com o Sentry, criando spans no dashboard **AI Insights** para monitoramento de:

- **Prompts e respostas** (inputs/outputs do LLM)
- **Uso de tokens** (input/output/total)
- **LatÃªncia** de chamadas
- **Taxa de erros** do Gemini
- **Performance** por modelo

## ðŸš€ InstalaÃ§Ã£o e ConfiguraÃ§Ã£o

### 1. A integraÃ§Ã£o jÃ¡ estÃ¡ ativa!

A integraÃ§Ã£o foi inicializada automaticamente em `/src/services/error-tracking.ts`:

```typescript
import { initGeminiIntegration } from '@/lib/sentry-gemini-integration';

// ApÃ³s Sentry.init()
initGeminiIntegration({
  includePrompts: true,  // Captura prompts e outputs
  captureErrors: true    // Captura erros automaticamente
});
```

### 2. ConfiguraÃ§Ã£o de PII (Personally Identifiable Information)

O Sentry considera **inputs/outputs de LLMs como PII** por padrÃ£o. Para incluir esses dados:

**OpÃ§Ã£o 1: Habilitar globalmente no Sentry**
```typescript
Sentry.init({
  // ...outras configs
  send_default_pii: true  // Envia PII por padrÃ£o
});
```

**OpÃ§Ã£o 2: Configurar na integraÃ§Ã£o Gemini**
```typescript
initGeminiIntegration({
  includePrompts: true,  // âœ… ForÃ§a captura mesmo sem send_default_pii
  captureErrors: true
});
```

**OpÃ§Ã£o 3: Desabilitar prompts explicitamente**
```typescript
initGeminiIntegration({
  includePrompts: false,  // âŒ NÃ£o captura prompts/outputs
  captureErrors: true     // âœ… Mas ainda captura erros
});
```

## ðŸ”§ Como Usar

### âœ… Uso AutomÃ¡tico (Recomendado)

Todas as funÃ§Ãµes do `gemini-service.ts` jÃ¡ estÃ£o instrumentadas automaticamente:

```typescript
import { callGemini } from '@/lib/gemini-service';

// âœ… Automaticamente instrumentado
const response = await callGemini("Analise este processo jurÃ­dico", {
  model: "gemini-2.5-pro",
  temperature: 0.7,
  maxOutputTokens: 4096
});

// Span criado automaticamente no Sentry com:
// - gen_ai.system: "gcp.gemini"
// - gen_ai.request.model: "gemini-2.5-pro"
// - gen_ai.operation.name: "chat"
// - gen_ai.request.messages: [{"role":"user", "content":"..."}]
// - gen_ai.response.text: ["resposta do modelo"]
// - gen_ai.usage.total_tokens: 1234
```

### ðŸŽ¯ Uso Manual (Casos EspecÃ­ficos)

Se vocÃª fizer chamadas diretas Ã  API do Gemini (fora do `gemini-service`), use o wrapper manual:

```typescript
import { instrumentGeminiCall } from '@/lib/sentry-gemini-integration';

async function myCustomGeminiCall() {
  const wrapper = instrumentGeminiCall(
    {
      model: "gemini-2.5-pro",
      operation: "generate_content",
      prompt: "Meu prompt personalizado",
      temperature: 0.8,
      maxTokens: 2048,
      startTime: Date.now()
    },
    {
      includePrompts: true,
      captureErrors: true
    }
  );

  return await wrapper(async () => {
    // Sua chamada customizada ao Gemini
    const response = await fetch(...);
    return response.json();
  });
}
```

### ðŸª React Hook

Para componentes React, use o hook `useGeminiInstrumentation`:

```typescript
import { useGeminiInstrumentation } from '@/lib/sentry-gemini-integration';

function MyComponent() {
  const { wrapGeminiCall } = useGeminiInstrumentation({
    includePrompts: true
  });

  const handleAnalyze = async () => {
    const result = await wrapGeminiCall(
      {
        model: "gemini-2.5-pro",
        prompt: "Analise este documento",
        temperature: 0.7
      },
      async () => {
        return await callGeminiAPI();
      }
    );
  };
}
```

### ðŸŽ¨ Decorator Pattern

Para funÃ§Ãµes que sempre usam Gemini, use o decorator:

```typescript
import { withGeminiInstrumentation } from '@/lib/sentry-gemini-integration';

const analyzeDocument = withGeminiInstrumentation(
  async (documentText: string) => {
    // LÃ³gica de anÃ¡lise
    return await callGeminiAPI(documentText);
  },
  {
    model: "gemini-2.5-pro",
    getPrompt: (text) => `Analise: ${text}`,
    temperature: 0.7
  },
  { includePrompts: true }
);

// Uso
const result = await analyzeDocument("Texto do processo...");
// âœ… Automaticamente instrumentado
```

## ðŸ“ˆ Visualizar Dados no Sentry

### 1. AI Insights Dashboard

Acesse: `https://sentry.io/organizations/[org]/insights/ai/agents/`

**MÃ©tricas disponÃ­veis:**
- ðŸ“Š **Token Usage**: Custo total de tokens por modelo
- â±ï¸ **Latency**: Tempo mÃ©dio de resposta do Gemini
- âŒ **Error Rate**: Taxa de falhas nas chamadas
- ðŸ“ˆ **Request Volume**: Volume de chamadas ao longo do tempo

### 2. Traces

Acesse: `https://sentry.io/organizations/[org]/performance/trace/[trace-id]`

Cada chamada Gemini aparece como:
```
gen_ai.chat gemini-2.5-pro
â”œâ”€ Attributes:
â”‚  â”œâ”€ gen_ai.system: gcp.gemini
â”‚  â”œâ”€ gen_ai.request.model: gemini-2.5-pro
â”‚  â”œâ”€ gen_ai.operation.name: chat
â”‚  â”œâ”€ gen_ai.request.messages: [{"role":"user",...}]
â”‚  â”œâ”€ gen_ai.request.temperature: 0.7
â”‚  â”œâ”€ gen_ai.request.max_tokens: 4096
â”‚  â”œâ”€ gen_ai.response.text: ["Resposta do modelo..."]
â”‚  â”œâ”€ gen_ai.usage.input_tokens: 150
â”‚  â”œâ”€ gen_ai.usage.output_tokens: 500
â”‚  â””â”€ gen_ai.usage.total_tokens: 650
```

### 3. Filtros Ãšteis

```sql
-- Chamadas ao Gemini
op:"gen_ai.chat" AND gen_ai.system:"gcp.gemini"

-- Chamadas lentas (>5s)
op:"gen_ai.chat" AND duration:>5000

-- Erros do Gemini
op:"gen_ai.chat" AND status:internal_error

-- Por modelo
gen_ai.request.model:"gemini-2.5-pro"

-- Alto uso de tokens
gen_ai.usage.total_tokens:>1000
```

## ðŸ” Atributos Capturados

### Atributos Comuns
| Atributo | Tipo | DescriÃ§Ã£o | Exemplo |
|----------|------|-----------|---------|
| `gen_ai.system` | string | Sistema de IA | `"gcp.gemini"` |
| `gen_ai.request.model` | string | Modelo usado | `"gemini-2.5-pro"` |
| `gen_ai.operation.name` | string | Tipo de operaÃ§Ã£o | `"chat"` |

### Atributos de RequisiÃ§Ã£o
| Atributo | Tipo | DescriÃ§Ã£o | Exemplo |
|----------|------|-----------|---------|
| `gen_ai.request.messages` | string | JSON das mensagens | `"[{\"role\":\"user\",\"content\":\"...\"}]"` |
| `gen_ai.request.temperature` | number | Temperatura do modelo | `0.7` |
| `gen_ai.request.max_tokens` | number | Limite de tokens | `4096` |

### Atributos de Resposta
| Atributo | Tipo | DescriÃ§Ã£o | Exemplo |
|----------|------|-----------|---------|
| `gen_ai.response.text` | string | JSON da resposta | `"[\"AnÃ¡lise completa...\"]"` |
| `gen_ai.usage.input_tokens` | number | Tokens de entrada | `150` |
| `gen_ai.usage.output_tokens` | number | Tokens de saÃ­da | `500` |
| `gen_ai.usage.total_tokens` | number | Total de tokens | `650` |

## ðŸ›¡ï¸ SeguranÃ§a e Privacidade

### PII (Personally Identifiable Information)

âš ï¸ **IMPORTANTE**: Prompts e respostas do Gemini podem conter dados sensÃ­veis de clientes.

**OpÃ§Ãµes de privacidade:**

1. **Desabilitar captura de prompts**
```typescript
setGeminiIntegrationOptions({ includePrompts: false });
```

2. **Filtrar dados sensÃ­veis antes de enviar**
```typescript
Sentry.init({
  beforeSend(event) {
    // Remover dados sensÃ­veis
    if (event.contexts?.gemini?.prompt) {
      event.contexts.gemini.prompt = "[REDACTED]";
    }
    return event;
  }
});
```

3. **Usar Sentry Data Scrubbing**
Configure regras de scrubbing no dashboard Sentry para remover automaticamente:
- CPFs
- CNPJs
- E-mails
- Nomes de clientes
- NÃºmeros de processos

## ðŸ“Š MÃ©tricas e Alertas

### Configurar Alertas

**1. Alto uso de tokens (custo)**
```
Condition: SUM(gen_ai.usage.total_tokens) > 100000
Period: 1 hour
Action: Email team
```

**2. LatÃªncia alta**
```
Condition: AVG(duration) > 5000ms
Period: 5 minutes
Action: Slack notification
```

**3. Taxa de erro elevada**
```
Condition: ERROR_RATE(gen_ai.chat) > 0.05
Period: 10 minutes
Action: PagerDuty
```

## ðŸ§ª Verificar IntegraÃ§Ã£o

Execute este teste para verificar se a integraÃ§Ã£o estÃ¡ funcionando:

```typescript
import { callGemini } from '@/lib/gemini-service';
import * as Sentry from '@sentry/react';

async function testGeminiIntegration() {
  const transaction = Sentry.startTransaction({
    name: "Test Gemini Integration",
    op: "test"
  });

  Sentry.getCurrentScope().setSpan(transaction);

  try {
    // Use texto real e sanitizado
    const documentText = getSanitizedDocumentText();
    const response = await callGemini(documentText, {
      model: "gemini-2.5-pro",
      temperature: 0.7
    });

    console.log("âœ… Gemini response:", response.text);
  } finally {
    transaction.finish();
  }
}

// Executar teste
testGeminiIntegration();
```

ApÃ³s executar, verifique:
1. **AI Spans tab**: `Explore > Traces > AI Spans`
2. **Performance**: Deve aparecer span `gen_ai.chat gemini-2.5-pro`
3. **Atributos**: Verificar se `gen_ai.request.messages` e `gen_ai.response.text` estÃ£o preenchidos

â±ï¸ **Nota**: Pode levar 1-2 minutos para os dados aparecerem no Sentry.

## ðŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### Alterar configuraÃ§Ã£o em runtime

```typescript
import { 
  setGeminiIntegrationOptions, 
  getGeminiIntegrationOptions 
} from '@/lib/sentry-gemini-integration';

// Ver configuraÃ§Ã£o atual
const config = getGeminiIntegrationOptions();
console.log(config); // { includePrompts: true, captureErrors: true }

// Alterar configuraÃ§Ã£o
setGeminiIntegrationOptions({
  includePrompts: false  // Desabilitar captura de prompts
});
```

### IntegraÃ§Ã£o com agentes autÃ´nomos

A integraÃ§Ã£o funciona automaticamente com o sistema de agentes:

```typescript
// src/lib/real-agent-client.ts jÃ¡ usa:
import { startAgentInvokeSpan, startAIChatSpan } from './sentry-ai-monitoring';

// âœ… Agentes automaticamente monitorados
// âœ… Chamadas Gemini dentro de agentes rastreadas
// âœ… Hierarquia de spans preservada (agent > chat)
```

## ðŸ“š ReferÃªncias

- [Sentry AI Monitoring Docs](https://docs.sentry.io/product/insights/ai/agents/)
- [Google Gen AI Python Integration](https://docs.sentry.io/platforms/python/integrations/google-genai/)
- [OpenTelemetry Gen AI Conventions](https://opentelemetry.io/docs/specs/semconv/gen-ai/)
- [CÃ³digo fonte: `/src/lib/sentry-gemini-integration.ts`](/src/lib/sentry-gemini-integration.ts)

---

**Ãšltima atualizaÃ§Ã£o**: 5 de dezembro de 2025  
**VersÃ£o**: 1.0.0  
**Autor**: Assistente JurÃ­dico PJe Team
