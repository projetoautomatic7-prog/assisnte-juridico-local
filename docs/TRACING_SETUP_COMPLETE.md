# âœ… Tracing OpenTelemetry - Setup Completo

> **Status**: ImplementaÃ§Ã£o concluÃ­da com sucesso  
> **Data**: 14 de dezembro de 2025  
> **VersÃ£o**: 1.0.1  
> **Endpoint OTLP**: `http://localhost:4319/v1/traces` (customizado)

## ðŸ“‹ Resumo da ImplementaÃ§Ã£o

O sistema de tracing OpenTelemetry foi completamente integrado ao Assistente JurÃ­dico PJe, permitindo observabilidade completa de:

- âœ… Chamadas LLM (Gemini 2.5 Pro)
- âœ… OperaÃ§Ãµes de agentes de IA
- âœ… Processamento de documentos jurÃ­dicos
- âœ… Busca vetorial (Qdrant)
- âœ… Workflows processuais

## ðŸŽ¯ O Que Foi Implementado

### 1. DependÃªncias Instaladas

Adicionadas ao `package.json`:

```json
{
  "@opentelemetry/api": "^1.9.0",
  "@opentelemetry/exporter-trace-otlp-http": "^0.54.2",
  "@opentelemetry/instrumentation": "^0.54.2",
  "@opentelemetry/resources": "^1.28.0",
  "@opentelemetry/sdk-trace-base": "^1.28.0",
  "@opentelemetry/sdk-trace-web": "^1.28.0",
  "@opentelemetry/semantic-conventions": "^1.28.0"
}
```

### 2. Arquivos Criados/Modificados

| Arquivo | Status | DescriÃ§Ã£o |
|---------|--------|-----------|
| [`src/lib/otel-integration.ts`](../src/lib/otel-integration.ts) | âœ… Criado | IntegraÃ§Ã£o OpenTelemetry + AI Toolkit |
| [`src/lib/tracing.ts`](../src/lib/tracing.ts) | âœ… Existente | Sistema de tracing compatÃ­vel com OpenTelemetry |
| [`src/lib/gemini-service.ts`](../src/lib/gemini-service.ts) | âœ… Modificado | Adicionado tracing em `callGemini()` |
| [`src/hooks/use-autonomous-agents.ts`](../src/hooks/use-autonomous-agents.ts) | âœ… Modificado | Tracing de agentes com `startAgentSpan()` |
| [`src/main.tsx`](../src/main.tsx) | âœ… Existente | InicializaÃ§Ã£o do OpenTelemetry |
| [`.env.example`](../.env.example) | âœ… Atualizado | VariÃ¡vel `VITE_OTLP_ENDPOINT` |

### 3. Fluxo de Tracing Implementado

```
AplicaÃ§Ã£o inicia
  â†“
main.tsx â†’ initializeOpenTelemetry()
  â†“
OpenTelemetry SDK registrado
  â†“
Agente executa tarefa
  â†“
startAgentSpan() cria span
  â†“
Chama Gemini via callGemini()
  â†“
startLLMSpan() cria span filho
  â†“
Executa chamada Ã  API
  â†“
endLLMSpan() com tokens/custo
  â†“
endAgentSpan() com resultado
  â†“
Span exportado para AI Toolkit (localhost:4318)
  â†“
VisualizaÃ§Ã£o no Trace Viewer
```

## ðŸš€ Como Usar

### 1. Instalar DependÃªncias

```bash
npm install
```

### 2. Configurar VariÃ¡veis de Ambiente

```bash
# .env.local
VITE_OTLP_ENDPOINT=http://localhost:4318/v1/traces
```

### 3. Abrir AI Toolkit Trace Viewer

1. Pressione `Ctrl+Shift+P` (Command Palette)
2. Execute: **"AI Toolkit: Open Trace Viewer"**
3. O viewer abrirÃ¡ em uma nova aba do VS Code

### 4. Iniciar AplicaÃ§Ã£o

```bash
npm run dev
```

### 5. Visualizar Traces

Conforme vocÃª usa o sistema:

- âœ… IntimaÃ§Ãµes sÃ£o processadas â†’ Span `agent.justine.process-intimation`
- âœ… Chamadas Gemini â†’ Span `llm.generateContent` com tokens
- âœ… Buscas Qdrant â†’ Span `vector.search`
- âœ… Agentes executam â†’ Span `agent.{id}.{operation}`

Os traces aparecerÃ£o **automaticamente** no AI Toolkit Trace Viewer.

## ðŸ“Š Spans Implementados

### Agentes de IA

```typescript
// Exemplo: Mrs. Justin-e processando intimaÃ§Ã£o
startAgentSpan('justine', 'Mrs. Justin-e', {
  sessionId: 'sess_123',
  attributes: {
    'expediente.id': 'exp_456',
    'processo.numero': '0001234-56.2024.5.01.0000'
  }
});
```

**Atributos capturados**:
- `agent.id`: ID do agente
- `agent.name`: Nome amigÃ¡vel
- `agent.session_id`: ID da sessÃ£o
- `expediente.id`: ID do expediente processado
- `processo.numero`: NÃºmero do processo

### Chamadas LLM (Gemini)

```typescript
// Chamada ao Gemini com tracing automÃ¡tico
const response = await callGemini(prompt, {
  temperature: 0.7,
  maxOutputTokens: 2048
});
```

**Atributos capturados**:
- `llm.model`: `gemini-2.5-pro`
- `llm.operation`: `generateContent`
- `llm.temperature`: Temperatura configurada
- `llm.max_tokens`: Limite de tokens
- `llm.prompt_tokens`: Tokens do prompt
- `llm.completion_tokens`: Tokens da resposta
- `llm.total_tokens`: Total de tokens
- `llm.duration_ms`: Tempo de resposta em ms

### Busca Vetorial (Qdrant)

```typescript
// Busca jurisprudencial com tracing
traceVectorSearch('legal_docs', 'search', async () => {
  return await qdrant.search({
    query: 'FGTS trabalhista',
    limit: 10
  });
}, {
  query: 'FGTS trabalhista',
  limit: 10,
  score_threshold: 0.7
});
```

## ðŸ” VisualizaÃ§Ã£o no AI Toolkit

### Exemplo de Trace Completo

```
ðŸ“Š Trace: Processar IntimaÃ§Ã£o CNJ-123
â”œâ”€ ðŸ¤– agent.justine.process-intimation [2.5s]
â”‚  â”œâ”€ ðŸ“ llm.generateContent [1.8s]
â”‚  â”‚  â”œâ”€ Prompt tokens: 450
â”‚  â”‚  â”œâ”€ Completion tokens: 320
â”‚  â”‚  â””â”€ Total: 770 tokens
â”‚  â”œâ”€ ðŸ” vector.search [0.5s]
â”‚  â”‚  â”œâ”€ Collection: legal_docs
â”‚  â”‚  â”œâ”€ Results: 5
â”‚  â”‚  â””â”€ Score: 0.85
â”‚  â””â”€ âœ… Status: OK
```

### Filtros DisponÃ­veis

- **Por Agente**: `agent.id:justine`
- **Por OperaÃ§Ã£o**: `llm.operation:generateContent`
- **Por Processo**: `processo.numero:0001234*`
- **Por DuraÃ§Ã£o**: `duration:>2000ms`
- **Por Status**: `status:error`

## ðŸŽ¯ Casos de Uso

### 1. Debugar Agente Lento

```typescript
// Ver qual agente estÃ¡ demorando
Traces â†’ Filtrar por "agent.*" â†’ Ordenar por duraÃ§Ã£o
```

### 2. Monitorar Custo de API

```typescript
// Ver consumo de tokens
Traces â†’ Filtrar por "llm.*" â†’ Somar tokens
```

### 3. Rastrear Erro em ProduÃ§Ã£o

```typescript
// Ver stack trace completo
Traces â†’ Filtrar por "status:error" â†’ Ver exception
```

### 4. Otimizar Performance

```typescript
// Identificar gargalos
Traces â†’ Flamegraph â†’ Ver operaÃ§Ãµes mais lentas
```

## ðŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### Endpoint Customizado

```typescript
// Para usar collector diferente (ex: Jaeger, Zipkin)
VITE_OTLP_ENDPOINT=http://seu-collector.com:4318/v1/traces
```

### Sampling Rate

```typescript
// src/lib/otel-integration.ts
tracingService.setSamplingRate(0.1); // 10% dos traces
```

### Desabilitar em ProduÃ§Ã£o

```typescript
// .env.production
VITE_OTLP_ENDPOINT= # Vazio = desabilitado
```

## ðŸ“š DocumentaÃ§Ã£o Adicional

- **Tracing Completo**: [`docs/TRACING.md`](./TRACING.md)
- **OpenTelemetry Docs**: https://opentelemetry.io/docs/
- **AI Toolkit**: https://github.com/microsoft/vscode-ai-toolkit

## âœ… Checklist de VerificaÃ§Ã£o

- [x] DependÃªncias OpenTelemetry instaladas
- [x] `initializeOpenTelemetry()` chamado em `main.tsx`
- [x] AI Toolkit Trace Viewer aberto
- [x] `VITE_OTLP_ENDPOINT` configurado
- [x] AplicaÃ§Ã£o rodando (`npm run dev`)
- [x] Traces aparecendo no viewer
- [x] Agentes gerando spans
- [x] Chamadas LLM capturando tokens
- [x] Erros sendo registrados

## ðŸŽ‰ PrÃ³ximos Passos

1. **Explorar Traces**: Execute operaÃ§Ãµes e veja os traces no AI Toolkit
2. **Customizar Spans**: Adicione atributos especÃ­ficos do seu domÃ­nio
3. **Configurar Alertas**: Use os dados para monitorar SLAs
4. **Otimizar Performance**: Identifique e resolva gargalos
5. **Integrar com ProduÃ§Ã£o**: Configure exportaÃ§Ã£o para Datadog/New Relic

---

**Implementado por**: GitHub Copilot Agent  
**Data**: 13 de dezembro de 2025  
**VersÃ£o do Sistema**: 1.0.1
