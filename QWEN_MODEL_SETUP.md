# Download do Modelo Qwen2.5-7B-Instruct

## ‚ö†Ô∏è Situa√ß√£o Atual

**Espa√ßo dispon√≠vel**: 5.7GB  
**Espa√ßo necess√°rio**: ~15GB  
**Status**: ‚ùå Espa√ßo insuficiente

## üìã Op√ß√µes Dispon√≠veis

### Op√ß√£o 1: Download Lite (Recomendado para agora) ‚úÖ

Baixa apenas arquivos de configura√ß√£o e tokenizador (~50MB):

```bash
python download-qwen-model-lite.py
```

**Vantagens**:
- R√°pido (~1 minuto)
- Requer pouco espa√ßo
- Permite testar configura√ß√£o

**Desvantagens**:
- N√£o inclui os pesos do modelo
- N√£o pode executar infer√™ncia local

---

### Op√ß√£o 2: Download Completo (Requer mais espa√ßo) ‚ö†Ô∏è

Baixa o modelo completo com todos os pesos:

```bash
python download-qwen-model.py
```

**Requisitos**:
- ~15GB de espa√ßo livre
- Conex√£o est√°vel (download pode levar horas)
- GPU recomendada para infer√™ncia

**Voc√™ precisaria liberar**: ~10GB de espa√ßo adicional

---

### Op√ß√£o 3: Usar API Remota (Recomendado) üåü

Em vez de baixar o modelo, use APIs remotas:

#### A. Hugging Face Inference API

```python
from huggingface_hub import InferenceClient

client = InferenceClient(token="seu_token_aqui")

response = client.text_generation(
    "Qwen/Qwen2.5-7B-Instruct",
    prompt="Ol√°, como voc√™ pode me ajudar?"
)
```

**Vantagens**:
- Sem uso de espa√ßo local
- Sem necessidade de GPU
- Sempre atualizado

**Custo**: Gr√°tis para uso limitado

#### B. Integra√ß√£o com Spark LLM (J√° configurado)

O projeto j√° usa Spark LLM (GPT-4) que √© mais poderoso:

```typescript
import * as spark from '@github/spark/llm'

const response = await spark.llm("Sua pergunta aqui", "gpt-4o")
```

---

## üöÄ Recomenda√ß√£o

**Para desenvolvimento local**: Use a Op√ß√£o 3B (Spark LLM j√° configurado)

**Para testes do Qwen**: Use a Op√ß√£o 3A (API Hugging Face)

**Para uso offline**: Libere espa√ßo e use a Op√ß√£o 2

---

## üìä Como Liberar Espa√ßo

Se quiser baixar o modelo completo, libere espa√ßo:

```bash
# Ver uso de espa√ßo
du -h --max-depth=1 /workspaces | sort -hr | head -20

# Limpar caches comuns
rm -rf ~/.cache/pip
rm -rf node_modules/.cache
docker system prune -a  # Se tiver permiss√£o

# Limpar builds antigos
rm -rf dist/
rm -rf build/
```

---

## üîó Links √öteis

- [Qwen2.5-7B-Instruct no HuggingFace](https://huggingface.co/Qwen/Qwen2.5-7B-Instruct)
- [Documenta√ß√£o Qwen](https://qwen.readthedocs.io/)
- [Hugging Face Inference API](https://huggingface.co/inference-api)
- [Spark LLM Documentation](https://githubnext.com/projects/spark)

---

## ‚ùì Perguntas Frequentes

**Q: Posso usar o modelo sem baixar?**  
A: Sim! Use a Hugging Face Inference API ou continue com Spark LLM.

**Q: Quanto tempo demora o download completo?**  
A: Depende da conex√£o. Pode levar de 30 minutos a v√°rias horas.

**Q: Preciso de GPU?**  
A: Para infer√™ncia local do modelo 7B, GPU √© altamente recomendada.

**Q: Qual modelo √© melhor?**  
A: Para produ√ß√£o, GPT-4 (via Spark) √© mais confi√°vel. Qwen √© bom para testes e experimenta√ß√£o.
